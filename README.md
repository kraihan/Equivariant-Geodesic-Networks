# Equivariant-Geodesic-Networks
<h2> Title: Equivariant Geodesic Networks: Geometry-Preserving  Learning on Riemannian Manifolds</h2>
<h4>Abstract: </h4> Many high-dimensional data modalities—including covariance descriptors, diffusion tensors, and kernel matrices—naturally reside on Riemannian manifolds such as the space of Symmetric Positive Definite (SPD) matrices. However, conventional deep neural networks often fail to respect the intrinsic geometry of such data, leading to suboptimal representations and generalization. We introduce Equivariant Geodesic Networks (EGN), a novel architecture designed to operate directly on Riemannian manifolds while preserving key geometric properties. EGN incorporates manifold-consistent operations, including equivariant mappings, adaptive geometric bias, and structured low-rank updates that respect the underlying topology. Unlike existing methods that either flatten or project SPD data into Euclidean
 space, EGN directly learns on the manifold, preserving geometric consistency throughout. We provide theoretical analysis of the manifold-preserving properties of our layers and demonstrate significant empirical gains on tasks involving SPD-valued data, such as EEG-based emotion recognition and imagined speech classification. EGN outperforms existing Euclidean and pseudo-manifold baselines, offering a principled approach to end-to-end learning on Riemannian data manifolds.
